{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparar el entorno y carga del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from keybert import KeyBERT\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "\n",
    "# üìå Mapea √≠ndice -> categor√≠a\n",
    "index_to_category = {\n",
    "    0: \"INDUSTRIAL\", 1: \"CIVIL\", 2: \"INFORM√ÅTICA\", 3: \"TELECOMUNICACIONES\",\n",
    "    4: \"ARQUITECTURA\", 5: \"FILOSOF√çA\", 6: \"PSICOLOG√çA\", 7: \"LETRAS\",\n",
    "    8: \"COMUNICACI√ìN SOCIAL\", 9: \"EDUCACI√ìN\", 10: \"ADMINISTRACI√ìN\",\n",
    "    11: \"CONTADUR√çA\", 12: \"RELACIONES INDUSTRIALES\", 13: \"SOCIOLOG√çA\",\n",
    "    14: \"ECONOM√çA\", 15: \"DERECHO\", 16: \"TEOLOG√çA\"\n",
    "}\n",
    "\n",
    "# üìå Configuraci√≥n del dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# üìå Cargar tokenizer y modelo BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'dccuchile/bert-base-spanish-wwm-cased',\n",
    "    num_labels=len(index_to_category)\n",
    ")\n",
    "model.load_state_dict(torch.load('best_model_state.bin', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()  # Modo inferencia\n",
    "\n",
    "# üìå Cargar modelo de KeyBERT\n",
    "kw_model = KeyBERT(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "\n",
    "# üìå Cargar modelo de SpaCy para espa√±ol\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "stopwords_es = list(nlp.Defaults.stop_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "####FUNCTIONS FOR TEST PONDERATION\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Si es una lista, unir los elementos en un solo string\n",
    "    if isinstance(text, list):\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s√°√©√≠√≥√∫√±√º]', '', text)  # Eliminar caracteres especiales, pero conservar tildes\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Reemplaza m√∫ltiples espacios\n",
    "    \n",
    "    # Procesar el texto con SpaCy sin eliminar todas las stopwords\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = \" \".join([token.text for token in doc if not token.is_punct])\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def extract_keywords(text, top_k=10):\n",
    "    \"\"\"\n",
    "    Extrae palabras clave utilizando KeyBERT y TF-IDF.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"el top k es \", top_k)\n",
    "    clean_text = preprocess_text(text)\n",
    "    keybert_keywords = kw_model.extract_keywords(clean_text, keyphrase_ngram_range=(1, 1), stop_words=stopwords_es, top_n=top_k)\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords_es)\n",
    "    tfidf_scores = vectorizer.fit_transform([clean_text]).toarray().flatten()\n",
    "    tfidf_tokens = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Crear un diccionario para mantener el score m√°s alto para cada palabra\n",
    "    keyword_scores = {}\n",
    "    \n",
    "    # Agregar scores de KeyBERT\n",
    "    for word, score in keybert_keywords:\n",
    "        keyword_scores[word] = score\n",
    "        \n",
    "    # # Agregar o actualizar con scores de TF-IDF\n",
    "    # for idx, token in enumerate(tfidf_tokens):\n",
    "    #     if token in keyword_scores:\n",
    "    #         keyword_scores[token] = max(keyword_scores[token], tfidf_scores[idx])\n",
    "    #     else:\n",
    "    #         keyword_scores[token] = tfidf_scores[idx]\n",
    "            \n",
    "    # Agregar/actualizar con scores de TF-IDF\n",
    "    for idx, token in enumerate(tfidf_tokens):\n",
    "        normal_token = token.strip().lower()\n",
    "        if normal_token in keyword_scores:\n",
    "            # Mantenemos el score m√°ximo\n",
    "            keyword_scores[normal_token] = max(keyword_scores[normal_token], tfidf_scores[idx])\n",
    "        else:\n",
    "            keyword_scores[normal_token] = tfidf_scores[idx]\n",
    "\n",
    "    # Ordenar por score y tomar los top_k √∫nicos\n",
    "    sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    unique_keywords = []\n",
    "    seen_words = set()\n",
    "\n",
    "    for word, score in sorted_keywords:\n",
    "        # 'word' ya est√° normalizada (sin espacios ni may√∫sculas)\n",
    "        if word not in seen_words:\n",
    "            seen_words.add(word)\n",
    "            unique_keywords.append((word, score))\n",
    "            if len(unique_keywords) == top_k:\n",
    "                break\n",
    "\n",
    "    print(\"unique keywords\", unique_keywords)\n",
    "    return unique_keywords\n",
    "\n",
    "def predict_career_with_keywords(text, tokenizer, model, index_to_category, device, max_length=512, top_k=15, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Preprocesa el texto, extrae palabras clave y realiza la predicci√≥n de carreras.\n",
    "    \"\"\"\n",
    "    print(\"ENTRANDO A KEYBERT \", text)\n",
    "    keywords = extract_keywords(text, top_k=20)\n",
    "    extracted_keywords = \" \".join([word for word, _ in keywords])\n",
    "    print(\"ENTRANDO A KEYBERT \")\n",
    "    # üîπ Concatenar palabras clave al texto original\n",
    "    enriched_text = f\"{text} {extracted_keywords}\"\n",
    "    print(enriched_text)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        [enriched_text],  \n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        scaled_logits = logits / temperature\n",
    "        probs = torch.softmax(scaled_logits, dim=1)\n",
    "        top_probs, top_indices = probs.topk(top_k, dim=1, largest=True, sorted=True)\n",
    "\n",
    "    batch_results = []\n",
    "    for i in range(len([text])):\n",
    "        row_probs = top_probs[i].cpu().numpy()\n",
    "        row_indices = top_indices[i].cpu().numpy()\n",
    "\n",
    "        result = []\n",
    "        for idx, p in zip(row_indices, row_probs):\n",
    "            category_name = index_to_category[idx]\n",
    "            result.append((category_name, float(p)))\n",
    "        batch_results.append(result)\n",
    "\n",
    "    return batch_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Palabras clave detectadas: con KEYBERT [('biolog√≠a educaci√≥n', 0.6218), ('pr√°cticos concretos', 0.5893), ('relatividad cu√°ntica', 0.5863), ('gustaba matem√°tica', 0.5696), ('matem√°tica pr√°ctico', 0.5915), ('cosas prefiero', 0.5629), ('pr√°ctico entend√≠as', 0.6024), ('ciencia tecnolog√≠a', 0.59), ('historia cultura', 0.58), ('teor√≠a relatividad', 0.5743), ('utilizarlos aplicaciones', 0.6088), ('tecnol√≥gico emprender', 0.5693), ('cultura acerca', 0.5718), ('modelos utilizarlos', 0.607), ('historia biolog√≠a', 0.6054)]\n",
      "Como desarrollador de IA, creo modelos para luego utilizarlos en aplicaciones. \n",
      "Me gustaba mucho la parte matem√°tica, por ser algo pr√°ctico una vez entend√≠as el tema. \n",
      "Sin embargo, tambi√©n me gustaba la materia de literatura, historia y biolog√≠a. \n",
      "Educaci√≥n f√≠sica, soy mala en los deportes. \n",
      "Si, un curso de ingl√©s, me gusta mucho el idioma. \n",
      "Me gusta ver series y pel√≠culas, tambi√©n ver documentales o videos para aprender cosas nuevas de ciencia, historia, cultura general. \n",
      "Ciencia, tecnolog√≠a, arte y cultura. \n",
      "Acerca de ciencia, como la teor√≠a de la relatividad, cu√°ntica, me gusta mucho entender el mundo desde esa perspectiva. \n",
      "Crear, investigar y resolver problemas. \n",
      "Me interesa entender c√≥mo funciona el mundo, Me gusta imaginar y crear cosas nuevas, Prefiero resolver problemas pr√°cticos y concretos. \n",
      "En un caf√©. \n",
      "Tecnol√≥gico. \n",
      "Emprender. mala deportes pel√≠culas documentales cultura acerca modelos utilizarlos entend√≠as tema cosas prefiero ciencia tecnolog√≠a teor√≠a relatividad educaci√≥n f√≠sica utilizarlos aplicaciones tecnol√≥gico emprender pr√°ctico entend√≠as relatividad cu√°ntica historia biolog√≠a pr√°cticos concretos gustaba matem√°tica matem√°tica pr√°ctico desarrollador ia biolog√≠a educaci√≥n historia cultura\n",
      "üîπ Texto Original:\n",
      "Como desarrollador de IA, creo modelos para luego utilizarlos en aplicaciones. \n",
      "Me gustaba mucho la parte matem√°tica, por ser algo pr√°ctico una vez entend√≠as el tema. \n",
      "Sin embargo, tambi√©n me gustaba la materia de literatura, historia y biolog√≠a. \n",
      "Educaci√≥n f√≠sica, soy mala en los deportes. \n",
      "Si, un curso de ingl√©s, me gusta mucho el idioma. \n",
      "Me gusta ver series y pel√≠culas, tambi√©n ver documentales o videos para aprender cosas nuevas de ciencia, historia, cultura general. \n",
      "Ciencia, tecnolog√≠a, arte y cultura. \n",
      "Acerca de ciencia, como la teor√≠a de la relatividad, cu√°ntica, me gusta mucho entender el mundo desde esa perspectiva. \n",
      "Crear, investigar y resolver problemas. \n",
      "Me interesa entender c√≥mo funciona el mundo, Me gusta imaginar y crear cosas nuevas, Prefiero resolver problemas pr√°cticos y concretos. \n",
      "En un caf√©. \n",
      "Tecnol√≥gico. \n",
      "Emprender.\n",
      "\n",
      "üîπ Palabras Clave Detectadas:\n",
      "[('biolog√≠a educaci√≥n', 0.6218), ('pr√°cticos concretos', 0.5893), ('relatividad cu√°ntica', 0.5863), ('gustaba matem√°tica', 0.5696), ('matem√°tica pr√°ctico', 0.5915), ('cosas prefiero', 0.5629), ('pr√°ctico entend√≠as', 0.6024), ('ciencia tecnolog√≠a', 0.59), ('historia cultura', 0.58), ('teor√≠a relatividad', 0.5743), ('utilizarlos aplicaciones', 0.6088), ('tecnol√≥gico emprender', 0.5693), ('cultura acerca', 0.5718), ('modelos utilizarlos', 0.607), ('historia biolog√≠a', 0.6054)]\n",
      "\n",
      "üîπ Predicciones Mejoradas:\n",
      "INFORM√ÅTICA: 51.09%\n",
      "FILOSOF√çA: 10.86%\n",
      "SOCIOLOG√çA: 6.11%\n"
     ]
    }
   ],
   "source": [
    "## PRUEBA DE LA FUNCION DE PONDERACION CON EL MODELO KEYBERT \n",
    "\n",
    "# Texto filtrado y concatenado\n",
    "user_text = \"\"\"Como desarrollador de IA, creo modelos para luego utilizarlos en aplicaciones. \n",
    "Me gustaba mucho la parte matem√°tica, por ser algo pr√°ctico una vez entend√≠as el tema. \n",
    "Sin embargo, tambi√©n me gustaba la materia de literatura, historia y biolog√≠a. \n",
    "Educaci√≥n f√≠sica, soy mala en los deportes. \n",
    "Si, un curso de ingl√©s, me gusta mucho el idioma. \n",
    "Me gusta ver series y pel√≠culas, tambi√©n ver documentales o videos para aprender cosas nuevas de ciencia, historia, cultura general. \n",
    "Ciencia, tecnolog√≠a, arte y cultura. \n",
    "Acerca de ciencia, como la teor√≠a de la relatividad, cu√°ntica, me gusta mucho entender el mundo desde esa perspectiva. \n",
    "Crear, investigar y resolver problemas. \n",
    "Me interesa entender c√≥mo funciona el mundo, Me gusta imaginar y crear cosas nuevas, Prefiero resolver problemas pr√°cticos y concretos. \n",
    "En un caf√©. \n",
    "Tecnol√≥gico. \n",
    "Emprender.\"\"\"\n",
    "\n",
    "#user_text = \"\"\"Desde muy joven, he sentido una gran pasi√≥n por la tecnolog√≠a. Me encanta aprender y desarrollar soluciones de software que resuelvan problemas reales; por ello, he orientado mis estudios hacia la Ingenier√≠a Inform√°tica. Adem√°s, me fascina el mundo de las telecomunicaciones, ya que creo que conectar a las personas a trav√©s de redes modernas y eficientes es clave para el avance social y econ√≥mico. Por otro lado, tambi√©n me interesa la parte gerencial y organizativa, lo que me lleva a valorar la Administraci√≥n; considero esencial saber planificar, gestionar proyectos y liderar equipos para llevar adelante iniciativas tecnol√≥gicas. En resumen, mi formaci√≥n y mis intereses se centran en el desarrollo de sistemas inform√°ticos, la conectividad a trav√©s de telecomunicaciones y la gesti√≥n estrat√©gica en entornos empresariales.\"\"\"\n",
    "#user_text = \"\"\"Estoy interesado en Ingenier√≠a Industrial. Fui becado y congel√© mi carrera en 2020 por temas econ√≥micos. Me gustaban las materias de Matem√°tica e Historia de Venezuela. No me agradaban Biolog√≠a y F√≠sica. Me interesa resolver problemas y explorar culturas. Me gustar√≠a ejercer mi carrera en Espa√±a y viajar por el mundo.\"\"\"\n",
    "# user_text = \"Me interesa trabajar con computadoras y desarrollar aplicaciones de software.\"\n",
    "\n",
    "\n",
    "# üîπ Prueba con el texto deusuario (PRUEBA KEYBERT)\n",
    "keywords = extract_keywords(user_text, top_k=15)\n",
    "print(\"üîπ Palabras clave detectadas: con KEYBERT\", keywords)\n",
    "\n",
    "\n",
    "\n",
    "# üîπ Realizar predicci√≥n con palabras clave a√±adidas\n",
    "predictions = predict_career_with_keywords(\n",
    "    text=user_text,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    index_to_category=index_to_category,\n",
    "    device=device,\n",
    "    temperature=1.3,\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(\"üîπ Texto Original:\")\n",
    "print(user_text)\n",
    "print(\"\\nüîπ Palabras Clave Detectadas:\")\n",
    "print(extract_keywords(user_text, top_k=15))\n",
    "print(\"\\nüîπ Predicciones Mejoradas:\")\n",
    "for career, prob in predictions[0]:\n",
    "    print(f\"{career}: {prob:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original:\n",
      "Como desarrollador de IA, creo modelos para luego utilizarlos en aplicaciones. \n",
      "Me gustaba mucho la parte matem√°tica, por ser algo pr√°ctico una vez entend√≠as el tema. \n",
      "Sin embargo, tambi√©n me gustaba la materia de literatura, historia y biolog√≠a. \n",
      "Educaci√≥n f√≠sica, soy mala en los deportes. \n",
      "Si, un curso de ingl√©s, me gusta mucho el idioma. \n",
      "Me gusta ver series y pel√≠culas, tambi√©n ver documentales o videos para aprender cosas nuevas de ciencia, historia, cultura general. \n",
      "Ciencia, tecnolog√≠a, arte y cultura. \n",
      "Acerca de ciencia, como la teor√≠a de la relatividad, cu√°ntica, me gusta mucho entender el mundo desde esa perspectiva. \n",
      "Crear, investigar y resolver problemas. \n",
      "Me interesa entender c√≥mo funciona el mundo, Me gusta imaginar y crear cosas nuevas, Prefiero resolver problemas pr√°cticos y concretos. \n",
      "En un caf√©. \n",
      "Tecnol√≥gico. \n",
      "Emprender.\n",
      "\n",
      "Texto preprocesado:\n",
      "como desarrollador de ia creo modelos para luego utilizarlos en aplicaciones me gustaba mucho la parte matem√°tica por ser algo pr√°ctico una vez entend√≠as el tema sin embargo tambi√©n me gustaba la materia de literatura historia y biolog√≠a educaci√≥n f√≠sica soy mala en los deportes si un curso de ingl√©s me gusta mucho el idioma me gusta ver series y pel√≠culas tambi√©n ver documentales o videos para aprender cosas nuevas de ciencia historia cultura general ciencia tecnolog√≠a arte y cultura acerca de ciencia como la teor√≠a de la relatividad cu√°ntica me gusta mucho entender el mundo desde esa perspectiva crear investigar y resolver problemas me interesa entender c√≥mo funciona el mundo me gusta imaginar y crear cosas nuevas prefiero resolver problemas pr√°cticos y concretos en un caf√© tecnol√≥gico emprender\n",
      "\n",
      "Predicciones:\n",
      "INFORM√ÅTICA: 38.56%\n",
      "FILOSOF√çA: 23.21%\n",
      "LETRAS: 5.74%\n"
     ]
    }
   ],
   "source": [
    "# // ... existing code ...\n",
    "\n",
    "\n",
    "# Cargar el modelo de SpaCy para espa√±ol\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def predict_career_with_preprocessing(text_list: list, tokenizer, model, \n",
    "                                   index_to_category: dict, device: str,\n",
    "                                   max_length: int = 128, top_k: int = 3, temperature: float = 1.0) -> list:\n",
    "    \"\"\"\n",
    "    Preprocesa el texto y realiza la predicci√≥n de carreras.\n",
    "    \n",
    "    Args:\n",
    "        text_list (list): Lista de textos a procesar\n",
    "        tokenizer: Tokenizer de BERT\n",
    "        model: Modelo BERT entrenado\n",
    "        index_to_category (dict): Mapeo de √≠ndices a nombres de carreras\n",
    "        device (str): Dispositivo para procesamiento ('cuda' o 'cpu')\n",
    "        max_length (int): Longitud m√°xima de tokens\n",
    "        top_k (int): N√∫mero de predicciones top a retornar\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de tuplas (carrera, probabilidad) para cada texto\n",
    "    \"\"\"\n",
    "    # Preprocesar cada texto\n",
    "    processed_texts = [preprocess_text(text) for text in text_list]\n",
    "    \n",
    "    # Tokenizar\n",
    "    inputs = tokenizer(\n",
    "        processed_texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Aplicar temperatura para suavizar las probabilidades\n",
    "        scaled_logits = logits / temperature\n",
    "        probs = torch.softmax(scaled_logits, dim=1)\n",
    "        \n",
    "        top_probs, top_indices = probs.topk(top_k, dim=1, largest=True, sorted=True)\n",
    "\n",
    "\n",
    "    batch_results = []\n",
    "    for i in range(len(text_list)):\n",
    "        row_probs = top_probs[i].cpu().numpy()\n",
    "        row_indices = top_indices[i].cpu().numpy()\n",
    "\n",
    "        result = []\n",
    "        for idx, p in zip(row_indices, row_probs):\n",
    "            category_name = index_to_category[idx]\n",
    "            result.append((category_name, float(p)))\n",
    "        batch_results.append(result)\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "# Ejemplo de uso:\n",
    "user_text = \"\"\"Como desarrollador de IA, creo modelos para luego utilizarlos en aplicaciones. \n",
    "Me gustaba mucho la parte matem√°tica, por ser algo pr√°ctico una vez entend√≠as el tema. \n",
    "Sin embargo, tambi√©n me gustaba la materia de literatura, historia y biolog√≠a. \n",
    "Educaci√≥n f√≠sica, soy mala en los deportes. \n",
    "Si, un curso de ingl√©s, me gusta mucho el idioma. \n",
    "Me gusta ver series y pel√≠culas, tambi√©n ver documentales o videos para aprender cosas nuevas de ciencia, historia, cultura general. \n",
    "Ciencia, tecnolog√≠a, arte y cultura. \n",
    "Acerca de ciencia, como la teor√≠a de la relatividad, cu√°ntica, me gusta mucho entender el mundo desde esa perspectiva. \n",
    "Crear, investigar y resolver problemas. \n",
    "Me interesa entender c√≥mo funciona el mundo, Me gusta imaginar y crear cosas nuevas, Prefiero resolver problemas pr√°cticos y concretos. \n",
    "En un caf√©. \n",
    "Tecnol√≥gico. \n",
    "Emprender.\"\"\"\n",
    "\n",
    "# user_text = \"\"\"Desde muy joven, he sentido una gran pasi√≥n por la tecnolog√≠a. Me encanta aprender y desarrollar soluciones de software que resuelvan problemas reales; por ello, he orientado mis estudios hacia la Ingenier√≠a Inform√°tica. Adem√°s, me fascina el mundo de las telecomunicaciones, ya que creo que conectar a las personas a trav√©s de redes modernas y eficientes es clave para el avance social y econ√≥mico. Por otro lado, tambi√©n me interesa la parte gerencial y organizativa, lo que me lleva a valorar la Administraci√≥n; considero esencial saber planificar, gestionar proyectos y liderar equipos para llevar adelante iniciativas tecnol√≥gicas. En resumen, mi formaci√≥n y mis intereses se centran en el desarrollo de sistemas inform√°ticos, la conectividad a trav√©s de telecomunicaciones y la gesti√≥n estrat√©gica en entornos empresariales.\"\"\"\n",
    "\n",
    "#user_text = \"\"\"Estoy interesado en Ingenier√≠a Industrial. Fui becado y congel√© mi carrera en 2020 por temas econ√≥micos. Me gustaban las materias de Matem√°tica e Historia de Venezuela. No me agradaban Biolog√≠a y F√≠sica. Me interesa resolver problemas y explorar culturas. Me gustar√≠a ejercer mi carrera en Espa√±a y viajar por el mundo.\"\"\"\n",
    "\n",
    "# Realizar predicci√≥n con el texto preprocesado\n",
    "predictions = predict_career_with_preprocessing(\n",
    "    text_list=[user_text],\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    index_to_category=index_to_category,\n",
    "    device=device,\n",
    "    temperature=1.3,\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "print(\"Texto original:\")\n",
    "print(user_text)\n",
    "print(\"\\nTexto preprocesado:\")\n",
    "print(preprocess_text(user_text))\n",
    "print(\"\\nPredicciones:\")\n",
    "for career, prob in predictions[0]:\n",
    "    print(f\"{career}: {prob:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
