{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '../data/raw/data_carrers.csv'\n",
    "OUTPUT_PATH = '../data/processed/processed-dataset.csv'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AdamW, get_cosine_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support,  confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Cargar el modelo de SpaCy para español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1) Modelo para traducir de español a inglés\n",
    "model_name_es_en = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "translator_es_en = pipeline(\n",
    "    \"translation_es_to_en\",\n",
    "    model=model_name_es_en,\n",
    "    tokenizer=model_name_es_en\n",
    ")\n",
    "\n",
    "# 2) Modelo para traducir de inglés a español\n",
    "model_name_en_es = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "translator_en_es = pipeline(\n",
    "    \"translation_en_to_es\",\n",
    "    model=model_name_en_es,\n",
    "    tokenizer=model_name_en_es\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "model_name = \"tuner007/pegasus_paraphrase\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def paraphrase_pegasus(input_text, num_return_sequences=3):\n",
    "    batch = tokenizer(\n",
    "        [input_text],\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    outputs = model.generate(\n",
    "        **batch,\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        temperature=1.5,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=2.5\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def manual_back_translation(text, translator_es_en, translator_en_es):\n",
    "    # 1) Traducir de español a inglés\n",
    "    result_en = translator_es_en(text, max_length=512, truncation=True)\n",
    "    # result_en es una lista de diccionarios [{'translation_text': \"...\"}]\n",
    "    text_en = result_en[0][\"translation_text\"]\n",
    "\n",
    "    # 2) Traducir la versión en inglés de vuelta a español\n",
    "    result_es = translator_en_es(text_en, max_length=512, truncation=True)\n",
    "    text_es = result_es[0][\"translation_text\"]\n",
    "\n",
    "    return text_es\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv(DATASET_PATH, encoding=\"UTF-8\")\n",
    "\n",
    "df_augmented_texts = []\n",
    "df_augmented_labels = []\n",
    "for idx, row in df.iterrows():\n",
    "    original_text = row['TEXTO']\n",
    "    label = row['CARRERA']\n",
    "\n",
    "    for _ in range(1):\n",
    "        # Ejecutar la traducción ida y vuelta\n",
    "        back_translated = manual_back_translation(\n",
    "            original_text,\n",
    "            translator_es_en,\n",
    "            translator_en_es\n",
    "        )\n",
    "        df_augmented_texts.append(back_translated)\n",
    "        df_augmented_labels.append(label)\n",
    "    # Genera, por ejemplo, 1 o 2 versiones back-translated\n",
    "    # ¡Ojo! Esto puede ser lento si tu dataset es grande\n",
    "    \n",
    "    text = translator_es_en(original_text, max_length=512, truncation=True) \n",
    "\n",
    "    paraphrased_texts = paraphrase_pegasus(text[0]['translation_text'])\n",
    "\n",
    "    for para_text in paraphrased_texts:\n",
    "        df_augmented_texts.append(translator_en_es(para_text, max_length=512, truncation=True)[0]['translation_text'] )\n",
    "        df_augmented_labels.append(label)\n",
    "        \n",
    "\n",
    "# Crear el df de ejemplos aumentados\n",
    "df_aug = pd.DataFrame({\n",
    "    'TEXTO': df_augmented_texts,\n",
    "    'CARRERA': df_augmented_labels\n",
    "})\n",
    "\n",
    "# Concatenar con el original\n",
    "df_final = pd.concat([df, df_aug]).reset_index(drop=True)\n",
    "\n",
    "df = df_final\n",
    "\n",
    "\n",
    "# Eliminar valores nulos y duplicados\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "# Preprocesar el texto\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\sáéíóúñü]', '', text)  # Elimina caracteres especiales pero mantiene caracteres acentuados\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Reemplaza múltiples espacios por uno solo\n",
    "    text = text.strip()  # Elimina espacios en blanco al inicio y al final\n",
    "    \n",
    "        # Procesar el texto con SpaCy para eliminar stopwords y lematizar\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "df['TEXTO'] = df['TEXTO'].apply(preprocess_text)\n",
    "\n",
    "# Mapear las categorías a índices numéricos\n",
    "print(df['CARRERA'].unique())\n",
    "categories = df['CARRERA'].unique().tolist()\n",
    "category_to_index = {category: idx for idx, category in enumerate(categories)}\n",
    "df['LABEL'] = df['CARRERA'].map(category_to_index)\n",
    "\n",
    "texts = df['TEXTO'].tolist()\n",
    "labels = df['LABEL'].tolist()\n",
    "\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "num_registros = df.shape[0]\n",
    "print(f\"Número total de registros en el dataset final: {num_registros}\")\n",
    "\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df['LABEL'].value_counts()\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "labels = torch.tensor(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribucion del dataset en diferentes conjuntos \n",
    "\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "# Convertir tensores a numpy arrays\n",
    "input_ids_np = input_ids.numpy()\n",
    "attention_mask_np = attention_mask.numpy()\n",
    "labels_np = labels.numpy()\n",
    "\n",
    "# División en entrenamiento+validación y prueba\n",
    "X_train_val, X_test, y_train_val, y_test, mask_train_val, mask_test = train_test_split(\n",
    "    input_ids_np, labels_np, attention_mask_np, test_size=0.2, random_state=42, stratify=labels_np)\n",
    "\n",
    "# División en entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val, mask_train, mask_val = train_test_split(\n",
    "    X_train_val, y_train_val, mask_train_val, test_size=0.25, random_state=42, stratify=y_train_val)\n",
    "\n",
    "print(y_train, y_val.shape, y_test.shape)\n",
    "# Convertir a tensores\n",
    "train_dataset = TensorDataset(torch.tensor(X_train,dtype=torch.long), torch.tensor(mask_train,dtype=torch.long), torch.tensor(y_train,dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val,dtype=torch.long), torch.tensor(mask_val,dtype=torch.long), torch.tensor(y_val,dtype=torch.long))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.long), torch.tensor(mask_test, dtype=torch.long), torch.tensor(y_test, dtype=torch.long))\n",
    "# Crear DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Configuracion del modelo bert y carga del modelo preentrenado\n",
    "config = BertConfig.from_pretrained('dccuchile/bert-base-spanish-wwm-cased', \n",
    "                                    num_labels=len(category_to_index), \n",
    "                                    hidden_dropout_prob=0.3, \n",
    "                                    attention_probs_dropout_prob=0.3)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-cased', config=config)\n",
    "\n",
    "## Definicion del optimizador AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5,eps=1e-8, weight_decay=0.01)\n",
    "\n",
    "## Balanceo de pesos de clases mediante la funcion compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "## Aleatoriedad de generacion de valores para las librerias \n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BUCLE DE ENTRENAMIENTO\n",
    "\n",
    "epochs = 20\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
    "patience = 1\n",
    "early_stopping_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Después de calcular class_weights\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "validation_accuracies = []\n",
    "training_accuracies = []\n",
    "training_f1_scores = []\n",
    "validation_f1_scores = [] \n",
    "learning_rates = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Entrenamiento ###\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_predictions = []\n",
    "    train_true_labels = []\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} - Entrenamiento\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Almacenar predicciones y etiquetas\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        train_predictions.extend(preds.cpu().numpy())\n",
    "        train_true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "    learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    training_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Calcular métricas de entrenamiento\n",
    "    train_accuracy = accuracy_score(train_true_labels, train_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    train_true_labels, train_predictions, average='weighted')\n",
    "    training_accuracies.append(train_accuracy)\n",
    "    training_f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    ### Validación ###\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_predictions = []\n",
    "    val_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_dataloader, desc=f\"Epoch {epoch+1}/{epochs} - Validación\"):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            val_predictions.extend(preds.cpu().numpy())\n",
    "            val_true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    validation_losses.append(avg_val_loss)\n",
    "\n",
    "    # Calcular precisión en validación\n",
    "    val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "    validation_accuracies.append(val_accuracy)\n",
    "    \n",
    "    # Después de calcular avg_val_loss y val_accuracy, agrega:\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        val_true_labels, val_predictions, average='weighted')\n",
    "    validation_f1_scores.append(f1)\n",
    "    \n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Pérdida Entrenamiento: {avg_train_loss:.4f}, Pérdida Validación: {avg_val_loss:.4f}, Precisión Validación: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Definir variables para Early Stopping\n",
    "    # Early Stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')  # Guarda el mejor modelo\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        print(f\"No hay mejora en la pérdida de validación. Paciencia: {early_stopping_counter}/{patience}\")\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early Stopping activado\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CELDA PARA EL REPORTE DE CLASIFICACION FINAL \n",
    "\n",
    "# Cargar el mejor modelo\n",
    "model.load_state_dict(torch.load('best_model_state.bin', weights_only=True))\n",
    "\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "test_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Evaluación en Conjunto de Prueba\"):\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        test_predictions.extend(preds.cpu().numpy())\n",
    "        test_true_labels.extend(labels.cpu().numpy()) \n",
    "\n",
    "# Generar el reporte de clasificación\n",
    "\n",
    "print(classification_report(test_true_labels, test_predictions, target_names=categories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "## MONITOREO DE GRÁFICOS Y METRICAS\n",
    "\n",
    "epochs_range = range(1, len(training_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Obtener la matriz de confusión en el conjunto de validación\n",
    "conf_matrix = confusion_matrix(val_true_labels, val_predictions)\n",
    "\n",
    "# Visualizar la matriz de confusión con un heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=categories, yticklabels=categories)\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Etiqueta Real')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.show()\n",
    "\n",
    "# Gráfica de tasa de aprendizaje\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(learning_rates) + 1), learning_rates, label=\"Tasa de Aprendizaje\")\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Tasa de Aprendizaje')\n",
    "plt.title('Curva de Tasa de Aprendizaje durante el Entrenamiento')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Gráfica de pérdidas\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, training_losses, label='Pérdida de Entrenamiento')\n",
    "plt.plot(epochs_range, validation_losses, label='Pérdida de Validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.title('Pérdida por Época')\n",
    "plt.show()\n",
    "\n",
    "# Gráfica de precisión\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, training_accuracies, label='Precisión de Entrenamiento')\n",
    "plt.plot(epochs_range, validation_accuracies, label='Precisión de Validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "plt.title('Precisión por Época')\n",
    "plt.show()\n",
    "\n",
    "# Gráfica de F1-score\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, training_f1_scores, label='F1-score de Entrenamiento')\n",
    "plt.plot(epochs_range, validation_f1_scores, label='F1-score de Validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('F1-score')\n",
    "plt.legend()\n",
    "plt.title('F1-score por Época')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
