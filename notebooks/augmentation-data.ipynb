{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = '../data/raw/data_carrers.csv'\n",
    "OUTPUT_PATH = '../data/processed/processed-dataset.csv'\n",
    "OUTPUT_PATH_SMOTE = '../data/processed/processed-dataset_smnote.csv'\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "import re\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "\n",
    "# Cargar el modelo de SpaCy para español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1) Modelo para traducir de español a inglés\n",
    "model_name_es_en = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "translator_es_en = pipeline(\n",
    "    \"translation_es_to_en\",\n",
    "    model=model_name_es_en,\n",
    "    tokenizer=model_name_es_en\n",
    ")\n",
    "\n",
    "# 2) Modelo para traducir de inglés a español\n",
    "model_name_en_es = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "translator_en_es = pipeline(\n",
    "    \"translation_en_to_es\",\n",
    "    model=model_name_en_es,\n",
    "    tokenizer=model_name_en_es\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "model_name = \"tuner007/pegasus_paraphrase\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def paraphrase_pegasus(input_text, num_return_sequences=2):\n",
    "    batch = tokenizer(\n",
    "        [input_text],\n",
    "        truncation=True,        padding=\"longest\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    outputs = model.generate(\n",
    "        **batch,\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        temperature=0.9,\n",
    "        top_k=30,\n",
    "        top_p=0.85,\n",
    "        repetition_penalty=2.5\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def manual_back_translation(text, translator_es_en, translator_en_es):\n",
    "    # 1) Traducir de español a inglés\n",
    "    result_en = translator_es_en(text, max_length=512, truncation=True)\n",
    "    # result_en es una lista de diccionarios [{'translation_text': \"...\"}]\n",
    "    text_en = result_en[0][\"translation_text\"]\n",
    "\n",
    "    # 2) Traducir la versión en inglés de vuelta a español\n",
    "    result_es = translator_en_es(text_en, max_length=512, truncation=True)\n",
    "    text_es = result_es[0][\"translation_text\"]\n",
    "\n",
    "    return text_es\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv(DATASET_PATH, encoding=\"UTF-8\")\n",
    "\n",
    "df_augmented_texts = []\n",
    "df_augmented_labels = []\n",
    "for idx, row in df.iterrows():\n",
    "    original_text = row['TEXTO']\n",
    "    label = row['CARRERA']\n",
    "\n",
    "    for _ in range(1):\n",
    "        # Ejecutar la traducción ida y vuelta\n",
    "        back_translated = manual_back_translation(\n",
    "            original_text,\n",
    "            translator_es_en,\n",
    "            translator_en_es\n",
    "        )\n",
    "        df_augmented_texts.append(back_translated)\n",
    "        df_augmented_labels.append(label)\n",
    "    # Genera, por ejemplo, 1 o 2 versiones back-translated\n",
    "    # ¡Ojo! Esto puede ser lento si tu dataset es grande\n",
    "    \n",
    "    text = translator_es_en(original_text, max_length=512, truncation=True) \n",
    "\n",
    "    paraphrased_texts = paraphrase_pegasus(text[0]['translation_text'])\n",
    "\n",
    "    for para_text in paraphrased_texts:\n",
    "        df_augmented_texts.append(translator_en_es(para_text, max_length=512, truncation=True)[0]['translation_text'] )\n",
    "        df_augmented_labels.append(label)\n",
    "        \n",
    "\n",
    "# Crear el df de ejemplos aumentados\n",
    "df_aug = pd.DataFrame({\n",
    "    'TEXTO': df_augmented_texts,\n",
    "    'CARRERA': df_augmented_labels\n",
    "})\n",
    "\n",
    "# Concatenar con el original\n",
    "df_final = pd.concat([df, df_aug]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "df = df_final\n",
    "\n",
    "\n",
    "# Eliminar valores nulos y duplicados\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "# Preprocesar el texto\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\sáéíóúñü]', '', text)  # Eliminar caracteres especiales, pero conservar tildes\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Reemplaza múltiples espacios\n",
    "    \n",
    "    # Procesar el texto con SpaCy sin eliminar todas las stopwords\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = \" \".join([token.text for token in doc if not token.is_punct])\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "df['TEXTO'] = df['TEXTO'].apply(preprocess_text)\n",
    "\n",
    "# Mapear las categorías a índices numéricos\n",
    "print(df['CARRERA'].unique())\n",
    "categories = df['CARRERA'].unique().tolist()\n",
    "category_to_index = {category: idx for idx, category in enumerate(categories)}\n",
    "df['LABEL'] = df['CARRERA'].map(category_to_index)\n",
    "\n",
    "################################SMOTE#######################\n",
    "\n",
    "# Aplicar SMOTE para balancear las clases\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(df['TEXTO'])\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_tfidf, df['LABEL'])\n",
    "\n",
    "df_balanced_texts = [\" \".join(lista) for lista in vectorizer.inverse_transform(X_resampled)]\n",
    "# Crear un nuevo DataFrame con los datos balanceados\n",
    "df_balanced = pd.DataFrame({\n",
    "    'TEXTO':df_balanced_texts,  # Convertir de nuevo a texto\n",
    "    'LABEL': y_resampled\n",
    "})\n",
    "\n",
    "# Guardar el dataset balanceado\n",
    "df_balanced.to_csv(OUTPUT_PATH_SMOTE, index=False)\n",
    "\n",
    "# Mostrar la nueva distribución de clases\n",
    "new_label_counts = df_balanced['LABEL'].value_counts()\n",
    "print(\"Distribución de clases después de SMOTE:\\n\", new_label_counts)\n",
    "\n",
    "\n",
    "df = df_balanced\n",
    "\n",
    "######################$#\n",
    "\n",
    "texts = df['TEXTO'].tolist()\n",
    "labels = df['LABEL'].tolist()\n",
    "\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "num_registros = df.shape[0]\n",
    "print(f\"Número total de registros en el dataset final: {num_registros}\")\n",
    "\n",
    "print(labels)\n",
    "\n",
    "#######################GENERACION DE TABLA DE MUESTRAS ANTES Y DESPUES ###################################\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# df_original: dataset antes de aplicar SMOTE\n",
    "# df_balanced: dataset después de aplicar SMOTE \n",
    "\n",
    "# Crear DataFrame de conteo antes de SMOTE\n",
    "before_counts = df_final['CARRERA'].value_counts().sort_index()\n",
    "# Crear DataFrame de conteo después de SMOTE\n",
    "after_counts = df_balanced['LABEL'].value_counts().sort_index()\n",
    "\n",
    "# Mapeo inverso de índice a nombre de carrera\n",
    "index_to_category = {v: k for k, v in category_to_index.items()}\n",
    "\n",
    "# Construir DataFrame comparativo\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Categoría\": [index_to_category[i] for i in after_counts.index],\n",
    "    \"Muestras Antes\": before_counts.values,\n",
    "    \"Muestras Después\": after_counts.values\n",
    "})\n",
    "\n",
    "print(summary_df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
